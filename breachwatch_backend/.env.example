
# Application Settings
APP_NAME="BreachWatch Backend"
LOG_LEVEL="INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Database Settings (PostgreSQL)
# These values are used by docker-compose.yml to initialize the DB
# and by the application to connect to it via DATABASE_URL.
# When running with docker-compose, DB_HOST *must* be the service name from docker-compose.yml (e.g., 'db').
DB_USER="breachwatchuser"
DB_PASSWORD="breachwatchpass"
DB_HOST="db" # CRITICAL: Use 'db' for Docker Compose internal networking. Use 'localhost' if running DB outside this Docker setup.
DB_PORT="5432"
DB_NAME="breachwatchdb"

# Assembled DATABASE_URL. Pydantic in config_loader.py will assemble this if individual components above are set.
# Alternatively, you can provide the full URL directly. Ensure it uses DB_HOST='db' for Docker.
# DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}

# Crawler Settings
DEFAULT_USER_AGENT="BreachWatchResearchBot/1.0 (Docker)"
REQUEST_TIMEOUT=20 # Increased timeout
# Path for downloaded files *inside the backend container*.
# This will be mapped to a local path via docker-compose volume mounts.
OUTPUT_LOCATIONS_DOWNLOADED_FILES="/app/data/downloaded_files" 
APP_BASE_DIR="/app" # Base directory inside the container

# Default target file extensions (these are primarily for default CrawlJob creation if not specified by API)
# Examples: ".txt",".csv",".sql",".json"
# TARGET_FILE_EXTENSIONS=

# Default keywords for analysis (primarily for default CrawlJob creation)
# Examples: "password","secret","api_key"
# KEYWORDS_FOR_ANALYSIS=
